from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import re
from threading import Lock
import requests
import hashlib
import os
import sys

app = Flask(__name__)
# ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ CORS Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø² Ø§ÙØ²ÙˆÙ†Ù‡ Ú©Ø±ÙˆÙ…
CORS(app)

FORBIDDEN_HOST = "melliun.org"
keyword_lock = Lock()
image_hash_lock = Lock()

# ğŸš¨ Ø´Ø¨ÛŒÙ‡ Ø³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: Ø§ÛŒÙ† Ù…ØªØºÛŒØ±Ù‡Ø§ Ù¾Ø³ Ø§Ø² Ù‡Ø± Ø¨Ø§Ø± Ø®Ø§Ù…ÙˆØ´ Ø´Ø¯Ù† Ø³Ø±ÙˆØ± (Cold Start) Ø±ÛŒØ³Øª Ù…ÛŒ Ø´ÙˆÙ†Ø¯.
CONTENT_HISTORY = {}
CONTENT_HISTORY_LOCK = Lock()

# ğŸš¨ ÙÙ‡Ø±Ø³Øª Ø¬Ø¯ÛŒØ¯: ÙÙ‡Ø±Ø³Øª Ù‡ÙØ´â€ŒÙ‡Ø§ÛŒ ØªØµØ§ÙˆÛŒØ± Ù…Ù…Ù†ÙˆØ¹Ù‡ (SHA256) Ú©Ù‡ Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.
# Ø¯Ø± Ù…Ø­ÛŒØ· ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ†Ù‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¯Ø± ÛŒÚ© Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø§Ø¦Ù…ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´ÙˆÙ†Ø¯.
FORBIDDEN_IMAGE_HASHES = set()

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù‡Ø¯Ø± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯Ù† ØªÙˆØ³Ø· Ø³Ø±ÙˆØ±Ù‡Ø§ÛŒ Ø¹Ú©Ø³
REQUEST_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (compatible; Content-Guard-Bot/1.0;)'
}
# Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ø± Ø¹Ú©Ø³
DOWNLOAD_TIMEOUT = 3

# ØªØ¹Ø±ÛŒÙ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©Ù„Ù…Ø§Øª Ù…Ù…Ù†ÙˆØ¹Ù‡ (Ú©Ù„Ù…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡) - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ Regex Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù…Ø±Ø² Ú©Ù„Ù…Ù‡
SENSITIVE_KEYWORDS = {
    r'(?:\s|^)Ø´ÙˆØ±Ø´', r'(?:\s|^)ØªØ­Ø±ÛŒÙ…', r'(?:\s|^)Ø¨Ø­Ø±Ø§Ù†', r'(?:\s|^)Ø³Ù‚ÙˆØ·',
    r'(?:\s|^)Ø¶Ø¯Ù†Ø¸Ø§Ù…', r'(?:\s|^)Ø§Ø¹ØªØ±Ø§Ø¶', r'(?:\s|^)Ø¨Ø±Ø§Ù†Ø¯Ø§Ø²',
    r'(?:\s|^)Ù‚ÛŒØ§Ù…', r'(?:\s|^)Ø¢Ø²Ø§Ø¯ÛŒ', r'(?:\s|^)Ø±Ù‡Ø¨Ø±', r'(?:\s|^)Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ',
    r'(?:\s|^)Ø§Ù†Ù‚Ù„Ø§Ø¨', r'(?:\s|^)Ø³Ù¾Ø§Ù‡', r'(?:\s|^)Ø¨Ø³ÛŒØ¬', r'(?:\s|^)Ú¯Ø´Øª\sØ§Ø±Ø´Ø§Ø¯',
    r'(?:\s|^)Ø³Ø±Ú©ÙˆØ¨', r'(?:\s|^)ÙØªÙ†Ù‡', r'(?:\s|^)Ø±Ú˜ÛŒÙ…', r'(?:\s|^)Ø¬Ù…Ù‡ÙˆØ±ÛŒ',
    r'(?:\s|^)Ø§Ø¹Ø¯Ø§Ù…', r'(?:\s|^)Ù†Ø¸Ø§Ù…', r'(?:\s|^)ÙˆÙ„Ø§ÛŒØª\sÙÙ‚ÛŒÙ‡', r'(?:\s|^)Ù…Ù„Ø§',
    r'(?:\s|^)Ù‚ÙˆÙ‡\sÙ‚Ø¶Ø§ÛŒÛŒÙ‡', r'(?:\s|^)Ø²Ù†Ø¯Ø§Ù†ÛŒ\sØ³ÛŒØ§Ø³ÛŒ', r'(?:\s|^)Ø¯ÛŒÚ©ØªØ§ØªÙˆØ±'
}


def get_image_hash(url):
    """
    ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø´ SHA256 Ø¢Ù†.
    Ø§Ú¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ù†Ø¨ÙˆØ¯ØŒ None Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    try:
        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† URLÙ‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹ØªØ¨Ø±
        if not url.startswith('http'):
            return None

            # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø­ØªÙˆØ§ÛŒ ØªØµÙˆÛŒØ± Ø¨Ø§ Ø§Ø³ØªØ±ÛŒÙ… Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø²Ù…Ø§Ù†ÛŒ
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=DOWNLOAD_TIMEOUT, stream=True)
        response.raise_for_status()  # Ø®Ø·Ø§Ù‡Ø§ÛŒ HTTP Ø±Ø§ Ù¾Ø±ØªØ§Ø¨ Ù…ÛŒ Ú©Ù†Ø¯

        # ÙÙ‚Ø· ØªØµØ§ÙˆÛŒØ± Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù† (Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø¨Ø§Ø±)
        content_type = response.headers.get('Content-Type', '')
        if 'image' not in content_type:
            return None

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø´ SHA256
        sha256_hash = hashlib.sha256()
        for chunk in response.iter_content(chunk_size=4096):
            sha256_hash.update(chunk)

        return sha256_hash.hexdigest()

    except requests.exceptions.RequestException:
        # Ø®Ø·Ø§ÛŒ Ø§ØªØµØ§Ù„ØŒ Timeout ÛŒØ§ Ø³Ø§ÛŒØ± Ù…Ø´Ú©Ù„Ø§Øª Ø¯Ø§Ù†Ù„ÙˆØ¯
        return None
    except Exception:
        # Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡
        return None


def normalize_text(text):
    """
    Ù…ØªÙ† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‚Øª ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ø¹Ø§Ø¯ÛŒâ€ŒØ³Ø§Ø²ÛŒ (Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²) Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹ ØªØ¨Ø¯ÛŒÙ„ ÛŒ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ).
    """
    text = str(text).lower()
    text = text.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©')
    # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± Regex
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def check_keyword_robust(article_text):
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‚ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Regex Ù¾Ø³ Ø§Ø² Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ.
    """
    normalized_text = normalize_text(article_text)

    with keyword_lock:
        for pattern in SENSITIVE_KEYWORDS:
            if re.search(pattern, normalized_text):
                return True
    return False


def simulate_learning(content_data):
    """
    Ø´Ø¨ÛŒÙ‡ Ø³Ø§Ø²ÛŒ Ù…Ú©Ø§Ù†ÛŒØ³Ù… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±: Ø§Ú¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ø§Ø² Ù…Ù†Ø¨Ø¹ Ù…Ù…Ù†ÙˆØ¹Ù‡ Ù¾ÛŒØ¯Ø§ Ø´ÙˆØ¯ØŒ
    Ú©Ù„Ù…Ø§Øª Ùˆ Ù‡ÙØ´â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ù„ÛŒØ³Øª ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒ Ú©Ù†Ø¯.
    """
    article_text = content_data.get('text', '')
    image_sources = content_data.get('imageSources', [])

    # 1. ØªØ´Ø®ÛŒØµ Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø´ (Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡ Ø³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³)
    normalized_text = normalize_text(article_text)
    # Ù‡Ø´ Ú©Ø±Ø¯Ù† 500 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø¨Ø§Ø±
    content_hash = hashlib.md5(normalized_text[:500].encode('utf-8')).hexdigest()

    with CONTENT_HISTORY_LOCK:
        if content_hash in CONTENT_HISTORY:
            # Ù…Ø­ØªÙˆØ§ ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø³ØªØŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.
            return 0, 0
        CONTENT_HISTORY[content_hash] = True

    # --- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ---
    # ÛŒØ§ÙØªÙ† Ú©Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ/Ø¹Ø±Ø¨ÛŒ Ø¨Ø§ Ø·ÙˆÙ„ 4 Ú©Ø§Ø±Ø§Ú©ØªØ± ÛŒØ§ Ø¨ÛŒØ´ØªØ±
    all_words = set(re.findall(r'[\u0600-\u06FF\u0750-\u077F]{4,}', normalized_text))
    newly_added_keywords = 0
    with keyword_lock:
        for word in all_words:
            safe_term = r'(?:\s|^)' + re.escape(word)  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù…Ø±Ø² Ú©Ù„Ù…Ù‡
            if safe_term not in SENSITIVE_KEYWORDS:
                SENSITIVE_KEYWORDS.add(safe_term)
                newly_added_keywords += 1

    # --- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡ÙØ´â€ŒÙ‡Ø§ÛŒ ØªØµØ§ÙˆÛŒØ± ---
    newly_added_hashes = 0
    with image_hash_lock:
        for src in image_sources:
            # ÙÙ‚Ø· URLÙ‡Ø§ÛŒÛŒ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ± Ú©Ù‡ Ø§Ø² FORBIDDEN_HOST Ù…ÛŒ Ø¢ÛŒÙ†Ø¯
            if FORBIDDEN_HOST in src:
                img_hash = get_image_hash(src)
                if img_hash and img_hash not in FORBIDDEN_IMAGE_HASHES:
                    FORBIDDEN_IMAGE_HASHES.add(img_hash)
                    newly_added_hashes += 1

    return newly_added_keywords, newly_added_hashes


def check_nested_api_logic(content_data):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ù…Ù†Ø·Ù‚ API ØªÙˆØ¯Ø±ØªÙˆÛŒ Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡ Ø¨Ø§ Ù…Ù†Ø·Ù‚ ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ ØªÙ‚ÙˆÛŒØª Ø´Ø¯Ù‡ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±.
    """
    article_text = content_data.get('text', '')
    links_to_check = content_data.get('links', [])
    image_sources = content_data.get('imageSources', [])

    # 0. Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
    has_forbidden_source = any(FORBIDDEN_HOST in src for src in image_sources) or any(
        FORBIDDEN_HOST in link for link in links_to_check)

    if has_forbidden_source:
        new_k, new_i = simulate_learning(content_data)
        if new_k > 0 or new_i > 0:
            print(f"AUTOMATIC LEARNING: Added {new_k} new keywords and {new_i} new image hashes.")
    # --- Ù¾Ø§ÛŒØ§Ù† Ù…Ú©Ø§Ù†ÛŒØ²Ù… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ---

    # 1. Ø¨Ø±Ø±Ø³ÛŒ Ù‡ÙØ´ ØªØµØ§ÙˆÛŒØ±: Ø¢ÛŒØ§ ØªØµÙˆÛŒØ± ÙØ¹Ù„ÛŒØŒ Ù‡ÙØ´ Ù…Ù…Ù†ÙˆØ¹Ù‡ Ø¯Ø§Ø±Ø¯ØŸ (Ù‚ÙˆÛŒâ€ŒØªØ±ÛŒÙ† ÙÛŒÙ„ØªØ± Ø¬Ø¯ÛŒØ¯)
    with image_hash_lock:
        for src in image_sources:
            current_hash = get_image_hash(src)
            if current_hash and current_hash in FORBIDDEN_IMAGE_HASHES:
                return {
                    "action": "FILTER_HARD",
                    "reason": "HIGH_PRIORITY: Known Forbidden Image Hash Detected"
                }

    # 2. Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ù…Ù†Ø¨Ø¹ Ù…Ù…Ù†ÙˆØ¹Ù‡ (Ø¨Ø±Ø§ÛŒ ØªØµÙˆÛŒØ± ÛŒØ§ Ù„ÛŒÙ†Ú©) - Ø§ÛŒÙ† ÛŒÚ© ÙÛŒÙ„ØªØ± Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø³Ø±ÛŒØ¹ Ø§Ø³Øª.
    if any(FORBIDDEN_HOST in src for src in image_sources):
        return {
            "action": "FILTER_HARD",
            "reason": "HIGH_PRIORITY: Image Source from Forbidden Host Detected (URL Match)"
        }

    # 3. Ù…Ù†Ø·Ù‚ Ù„ÛŒÙ†Ú© Ùˆ Ù…ØªÙ†
    has_forbidden_link = any(FORBIDDEN_HOST in link for link in links_to_check)

    if has_forbidden_link:
        if len(article_text) > 100 and check_keyword_robust(article_text):
            return {
                "action": "FILTER_HARD",
                "reason": "Nested Logic: Forbidden Link + Sensitive Topic Match (Robust)"
            }
        # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ù…Ù…Ù†ÙˆØ¹Ù‡ Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ø­Ø³Ø§Ø³ Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ø§Ø² Ù‡Ù… Ø³Ø®Øª Ù…Ø³Ø¯ÙˆØ¯ Ú©Ù† (Ø§Ø­ØªÛŒØ§Ø· Ø¨ÛŒØ´ØªØ±)
        return {
            "action": "FILTER_HARD",
            "reason": "HIGH_PRIORITY: Forbidden Link Detected"
        }

    # 4. ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ø³Ø¨Ú©â€ŒØªØ± (Ø¨Ø±Ø±Ø³ÛŒ ÙÙ‚Ø· Ù…ØªÙ†)
    if check_keyword_robust(article_text):
        return {"action": "FILTER_LIGHT", "reason": "Generic Sensitive Topic Found (Robust)"}

    return {"action": "ALLOW", "reason": "Content is clear."}


# Ù…Ø³ÛŒØ± Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³Ø±ÙˆØ±
@app.route('/', methods=['GET'])
def home():
    # Ù†Ù…Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ùˆ ØªØµØ§ÙˆÛŒØ± ÙÛŒÙ„ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ£ÛŒÛŒØ¯ ØµØ­Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
    total_images = 0
    with image_hash_lock:
        total_images = len(FORBIDDEN_IMAGE_HASHES)

    return f"Python Content Filter API is running! Total keywords: {len(SENSITIVE_KEYWORDS)}. Total forbidden image hashes: {total_images}", 200


@app.route('/analyze_content_api', methods=['POST'])
def analyze_content_api():
    """
    Ù†Ù‚Ø·Ù‡ Ù¾Ø§ÛŒØ§Ù†ÛŒ Ú©Ù‡ Ø§ÙØ²ÙˆÙ†Ù‡ Ú©Ø±ÙˆÙ… Ø¢Ù† Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙˆÚ©Ù† Ù†Ø¯Ø§Ø±Ø¯).
    """
    data = request.get_json()
    if not data or 'content' not in data:
        return jsonify({"error": "No content provided."}), 400

    result = check_nested_api_logic(data['content'])
    return jsonify(result)


# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
if __name__ == '__main__':
    # Ø§ÛŒÙ† Ø®Ø· ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ø§Ø³Øª.
    # Ø¯Ø± Ù…Ø­ÛŒØ· Ø§Ø¨Ø±ÛŒ (Ù…Ø«Ù„ Render) Ø§Ø¬Ø±Ø§ÛŒ Gunicorn Ø§Ø² $PORT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    # ØªÙ†Ø¸ÛŒÙ… host='0.0.0.0' Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ø±ÙˆÛŒ ØªÙ…Ø§Ù… Ø§ÛŒÙ†ØªØ±ÙÛŒØ³â€ŒÙ‡Ø§
    app.run(debug=True, host='0.0.0.0', port=os.environ.get('PORT', 5050))
